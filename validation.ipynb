{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc5fNQZ-AQAg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2mGg7nBJljn"
      },
      "outputs": [],
      "source": [
        "device_num = 2\n",
        "torch.cuda.set_device(device_num)\n",
        "device = f\"cuda:{device_num}\"\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYsOjGERAqNh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"test_dataset.csv\", delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lay8fD0hA3DG"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt:str, tokenizer:AutoTokenizer, model:AutoModelForCausalLM):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        response = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            num_beams=4,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "    output = tokenizer.decode(response[0])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CizGQBPBDdO"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsx2QOGjHTro"
      },
      "outputs": [],
      "source": [
        "peft_model = \"path-to-qlora-weights\"\n",
        "name = \"path-to-base-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deQneKGQHWOW"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(name, trust_remote_code=True, config=bnb_config)\n",
        "model = PeftModel.from_pretrained(model, peft_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98ZHWbcBBDuG"
      },
      "outputs": [],
      "source": [
        "result_df = pd.DataFrame()\n",
        "\n",
        "for ind, prompt in df.iterrows():\n",
        "    answer = generate_text(prompt, tokenizer, model)\n",
        "    result_df.loc[ind, f'{name}-answer'] = answer\n",
        "result_df.to_csv(\"res.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
