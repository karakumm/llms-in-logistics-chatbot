{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_hWE6aYPJs-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# load data\n",
        "test_df = pd.read_csv(\"test_dataset.csv\", delimiter=\"\\t\")\n",
        "res_df = pd.read_csv(\"result_df.csv\", delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAxbuxFTPFIF"
      },
      "outputs": [],
      "source": [
        "answers_df = pd.DataFrame({\n",
        "    'Questions': test_df['Question'],\n",
        "    'True answers': test_df['Answers'],\n",
        "    'Vicuna-7b': res_df['vicuna-7'],\n",
        "    'Llama2-13b': res_df['llama2-13'],\n",
        "    'Mistral-7b': res_df['mistral-7b'],\n",
        "    'Vicuna-13b': res_df['vicuna-13b'],\n",
        "    'Vicuna-33b': res_df['vicuna-33b'],\n",
        "    'Zephyr-7b': res_df['zephyr-7b']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HKCjRjeX_tR"
      },
      "source": [
        "# Calculate ROUGE, SacreBLEU, Perplexity, Toxicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roejQjXGK3cA"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "sacrebleu = load(\"sacrebleu\")\n",
        "rouge = load('rouge')\n",
        "toxicity = load(\"toxicity\", module_type=\"measurement\")\n",
        "perplexity = load(\"perplexity\", module_type=\"metric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VipGdEOOwr_"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, references):\n",
        "  sacrebleu_metric = sacrebleu.compute(predictions=predictions, references=[[a] for a in references])\n",
        "  rouge_metric = rouge.compute(predictions=predictions, references=references)\n",
        "  toxic_metric = toxicity.compute(predictions=predictions, aggregation=\"ratio\")\n",
        "  perplexity_metric = perplexity.compute(model_id='gpt2', predictions=predictions)\n",
        "\n",
        "  return sacrebleu_metric, rouge_metric, toxic_metric, perplexity_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pVzu4KLYSBX"
      },
      "outputs": [],
      "source": [
        "llama2_res = calculate_metrics(res_df['llama2-13'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBYpS-aTYkBH"
      },
      "outputs": [],
      "source": [
        "vicuna7_res = calculate_metrics(res_df['vicuna-7'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IGsWEJJYbN4"
      },
      "outputs": [],
      "source": [
        "vicuna_13b_res = calculate_metrics(res_df['vicuna-13b'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57fXfO4AYkUv"
      },
      "outputs": [],
      "source": [
        "vicuna_33b_res = calculate_metrics(res_df['vicuna-33b'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py49uSCRYkMF"
      },
      "outputs": [],
      "source": [
        "mistral_7b_res = calculate_metrics(res_df['mistral-7b'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o-LfZoiYkco"
      },
      "outputs": [],
      "source": [
        "zephyr_7b_res = calculate_metrics(res_df['zephyr-7b'], test_df['Answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qncaNseRZQXQ"
      },
      "outputs": [],
      "source": [
        "rows = [vicuna7_res, llama2_res, vicuna_13b_res, vicuna_33b_res, mistral_7b_res, zephyr_7b_res]\n",
        "column_names = ['sacrebleu', 'rouge', 'toxic', 'perplexity']\n",
        "row_names = ['vicuna_7b', 'llama2_13b', 'vicuna_13b', 'vicuna_33b' 'mistral_7b', 'zephyr_7b']\n",
        "df_res = pd.DataFrame(rows, columns=column_names, index=row_names)\n",
        "df_res.to_csv('metrics.csv' index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXLGSuvyZqRZ"
      },
      "source": [
        "# Calculate Cosine Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJFy1Mi_ZvYg"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upg1XEvkZxXv"
      },
      "outputs": [],
      "source": [
        "text_data = pd.concat([answers_df['Questions'], answers_df.drop(columns='Questions').stack()]).unique()\n",
        "text_embeddings = model.encode(text_data)\n",
        "embedding_dict = dict(zip(text_data, text_embeddings))\n",
        "\n",
        "for column in answers_df.columns[1:]:\n",
        "    similarities = []\n",
        "    for i, row in answers_df.iterrows():\n",
        "        question_embedding = embedding_dict[row['Questions']]\n",
        "        answer_embedding = embedding_dict[row[column]]\n",
        "        similarity = cosine_similarity([question_embedding], [answer_embedding])\n",
        "        similarities.append(similarity[0][0])\n",
        "    answers_df['Similarity with ' + column] = similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeyLhNCMZ7SW"
      },
      "outputs": [],
      "source": [
        "cos_sim = answers_df[['Similarity with Vicuna-7b','Similarity with Llama2-13b', 'Similarity with Mistral-7b', 'Similarity with Vicuna-13b', 'Similarity with Vicuna-33b', 'Similarity with Zephyr-7b']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UibT8OzBZ9eQ"
      },
      "outputs": [],
      "source": [
        "#Plot HeatMap\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=cos_sim,\n",
        "    x=[el[15:] for el in cos_sim.columns],\n",
        "    y=list(range(1, len(cos_sim))),\n",
        "    colorscale='ice',\n",
        "    zmin=0, zmax=1,\n",
        "    colorbar_title='Cosine Similarity'\n",
        "))\n",
        "fig.update_layout(\n",
        "    title='Cosine Similarities between True Answers and Model Predictions',\n",
        "    xaxis_title='LLMs',\n",
        "    xaxis_tickangle=45,\n",
        "    yaxis_title='Question Number',\n",
        "    width=600,\n",
        "    height=700,\n",
        "    plot_bgcolor='white'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gq7r0pqaO7n"
      },
      "outputs": [],
      "source": [
        "# Plot BoxPlots\n",
        "\n",
        "fig = go.Figure()\n",
        "colors = px.colors.qualitative.Plotly\n",
        "\n",
        "for i, col in enumerate(cos_sim.columns):\n",
        "    fig.add_trace(go.Box(y=cos_sim[col],\n",
        "                         name=col[15:],\n",
        "                         marker_color=colors[i % len(colors)],\n",
        "                         boxpoints='all',\n",
        "                         jitter=0.3,\n",
        "                         pointpos=-1.8,\n",
        "                         marker=dict(opacity=0.5) ))\n",
        "fig.update_layout(\n",
        "    title='Boxplot of Cosine Similarities for Each Model',\n",
        "    xaxis_title='Models',\n",
        "    yaxis_title='Cosine Similarity',\n",
        "    xaxis=dict(tickangle=45),\n",
        "    width=1000,\n",
        "    height=600\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpHc5W-OatwR"
      },
      "source": [
        "# LLM-as-a-Judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hgDYp6vaw2w"
      },
      "outputs": [],
      "source": [
        "# !pip install huggingface_hub datasets pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRssJ9GayIo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import InferenceClient, notebook_login\n",
        "\n",
        "tqdm.pandas()\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrv-h1qsazTI"
      },
      "outputs": [],
      "source": [
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=repo_id,\n",
        "    timeout=120,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSifsIDDa3IB"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = \"\"\"\n",
        "You will be given a set of user_question, true_answer and 6 alternative answers (alternative_answers) to this question.\n",
        "The titles of these six answers in input order are: Llama2-13b, Mistral-7b, Vicuna-7b, Vicuna-13b, Vicuna-33b, Zephyr-7b, use these titles later\n",
        "Your task is to evaluate and rank these six alternative answers for each entry. The ranking should be from best to worst based on how closely each answer matches the true answer in terms of accuracy, relevance, and completeness.\n",
        "\n",
        "To accomplish this, you should:\n",
        "\n",
        "Analyze the true answer to understand the core information and context.\n",
        "Compare each generated answer against the true answer, assessing factors such as correctness, detail, and how well it addresses the user's question.\n",
        "Rank the answers from 1 to 6, where 1 is the best answer that most accurately and completely reflects the true answer, and 6 is the least accurate and complete, like a list: 1st place - Mistral-7b, etc., use the titles of those answers that I have meantioned before.\n",
        "Your output for each entry should clearly indicate the ranking of the generated answers.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Evaluation: (your rationale for the ranking, as a text)\n",
        "Total ranking: (your ranking, as a list of titles of answers from best to worst)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total ranking:' in your answer.\n",
        "\n",
        "Now here are the question, true answer, and alternative answers.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {true_answer}\n",
        "Alternarives {alternative_answers}\n",
        "\n",
        "Provide your feedback.\n",
        "Feedback:::\n",
        "Evaluation: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRbQjHHsbHTJ"
      },
      "outputs": [],
      "source": [
        "answers_df[\"llm_judge_impr\"] = answers_df.progress_apply(\n",
        "    lambda answers_df: llm_client.text_generation(\n",
        "        prompt=JUDGE_PROMPT.format(question=answers_df[\"Questions\"], true_answer=answers_df[\"True answers\"], alternative_answers=(answers_df[\"Llama2-13b\"], answers_df[\"Mistral-7b\"], answers_df[\"Vicuna-7b\"], answers_df[\"Vicuna-13b\"], answers_df[\"Vicuna-33b\"], answers_df[\"Zephyr-7b\"])),\n",
        "        max_new_tokens=500,\n",
        "    ),\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFrwHLLVbNAZ"
      },
      "outputs": [],
      "source": [
        "answers_df.to_csv('judge.csv', sep='\\t', columns=[\"Questions\", \"llm_judge_impr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1elQFaUjbOkW"
      },
      "outputs": [],
      "source": [
        "first_place = []\n",
        "\n",
        "def extract_judge_score(text, current, next_delimiter=None):\n",
        "    start_split = text.split(current)\n",
        "    if next_delimiter:\n",
        "        end_split = start_split[1].split(next_delimiter)\n",
        "    else:\n",
        "        end_split = start_split[1].split(\"\\n\")\n",
        "    return end_split[0].strip()\n",
        "\n",
        "delimiters = [\"1st place - \", \"2nd place - \"]\n",
        "for text in list(answers_df[\"llm_judge_impr\"]):\n",
        "  first_place.append(extract_judge_score(text, delimiters[0], delimiters[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Neu6XYhbPAK"
      },
      "outputs": [],
      "source": [
        "# Plot Pie Chart\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "first_place = [model.strip().rstrip(',') for model in first_place]\n",
        "\n",
        "model_counts = {}\n",
        "for model in first_place:\n",
        "    if model in model_counts:\n",
        "        model_counts[model] += 1\n",
        "    else:\n",
        "        model_counts[model] = 1\n",
        "labels = list(model_counts.keys())\n",
        "sizes = list(model_counts.values())\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=sizes,\n",
        "                             textinfo='label+percent',\n",
        "                             insidetextorientation='radial')])\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text='LLM-as-a-judge top 1',\n",
        "    showlegend=True,\n",
        "    width=600,\n",
        "    height=600\n",
        ")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
